Timer unit: 1e-09 s

Total time: 0.555773 s
File: /home/luka/repo/JAXOvercooked/baselines/test.py
Function: _env_step at line 122

Line #      Hits         Time  Per Hit   % Time  Line Contents
==============================================================
   122                                                           @line_profiler.profile
   123                                                           def _env_step(runner_state, _):
   124         1        509.0    509.0      0.0                      env_state, last_obs, update_step, rng = runner_state
   125         1     741060.0 741060.0      0.1                      rng, key_a0, key_a1, key_s = jax.random.split(rng, 4)
   126         1     562696.0 562696.0      0.1                      action_0 = jnp.broadcast_to(sample_discrete_action(key_a0, env.action_space()), (config["NUM_ENVS"],))
   127         1     469533.0 469533.0      0.1                      action_1 = jnp.broadcast_to(sample_discrete_action(key_a1, env.action_space()), (config["NUM_ENVS"],))
   128         1        393.0    393.0      0.0                      actions = {"agent_0": action_0, "agent_1": action_1}
   129                                           
   130         2  553706620.0    3e+08     99.6                      obsv, env_state, reward, done, info = jax.vmap(env.step, in_axes=(0, 0, 0))(
   131         1     291187.0 291187.0      0.1                          jax.random.split(key_s, config["NUM_ENVS"]), env_state, actions
   132                                                               )
   133                                           
   134         1        413.0    413.0      0.0                      runner_state = (env_state, obsv, update_step, rng)
   135         1        768.0    768.0      0.0                      metrics = {"reward": reward["agent_0"], "done": done["__all__"]}
   136                                           
   137         1        163.0    163.0      0.0                      return runner_state, metrics

Total time: 1.78413 s
File: /home/luka/repo/JAXOvercooked/jax_marl/environments/overcooked_environment/overcooked.py
Function: step_agents at line 362

Line #      Hits         Time  Per Hit   % Time  Line Contents
==============================================================
   362                                               @line_profiler.profile
   363                                               def step_agents(
   364                                                       self, key: chex.PRNGKey, state: State, action: chex.Array,
   365                                               ) -> Tuple[State, float]:
   366                                           
   367                                                   # Update agent position (forward action)
   368        15    8838230.0 589215.3      0.5          is_move_action = jnp.logical_and(action != Actions.stay, action != Actions.interact)
   369        15    2790457.0 186030.5      0.2          is_move_action_transposed = jnp.expand_dims(is_move_action, 0).transpose()  # Necessary to broadcast correctly
   370                                           
   371        30    3547494.0 118249.8      0.2          fwd_pos = jnp.minimum(
   372        45   36144452.0 803210.0      2.0              jnp.maximum(state.agent_pos + is_move_action_transposed * DIR_TO_VEC[jnp.minimum(action, 3)] \
   373        30    4581490.0 152716.3      0.3                          + ~is_move_action_transposed * state.agent_dir, 0),
   374        15    2246607.0 149773.8      0.1              jnp.array((self.width - 1, self.height - 1), dtype=jnp.uint32)
   375                                                   )
   376                                           
   377                                                   # Can't go past wall or goal
   378        15       8271.0    551.4      0.0          def _wall_or_goal(fwd_position, wall_map, goal_pos):
   379                                                       fwd_wall = wall_map.at[fwd_position[1], fwd_position[0]].get()
   380                                                       goal_collision = lambda pos, goal : jnp.logical_and(pos[0] == goal[0], pos[1] == goal[1])
   381                                                       fwd_goal = jax.vmap(goal_collision, in_axes=(None, 0))(fwd_position, goal_pos)
   382                                                       # fwd_goal = jnp.logical_and(fwd_position[0] == goal_pos[0], fwd_position[1] == goal_pos[1])
   383                                                       fwd_goal = jnp.any(fwd_goal)
   384                                                       return fwd_wall, fwd_goal
   385                                           
   386        15  118844438.0    8e+06      6.7          fwd_pos_has_wall, fwd_pos_has_goal = jax.vmap(_wall_or_goal, in_axes=(0, None, None))(fwd_pos, state.wall_map, state.goal_pos)
   387                                           
   388        15    4332796.0 288853.1      0.2          fwd_pos_blocked = jnp.logical_or(fwd_pos_has_wall, fwd_pos_has_goal).reshape((self.num_agents, 1))
   389                                           
   390        15    4774174.0 318278.3      0.3          bounced = jnp.logical_or(fwd_pos_blocked, ~is_move_action_transposed)
   391                                           
   392                                                   # Agents can't overlap
   393                                                   # Hardcoded for 2 agents (call them Alice and Bob)
   394        15    1896854.0 126456.9      0.1          agent_pos_prev = jnp.array(state.agent_pos)
   395        15   15015046.0    1e+06      0.8          fwd_pos = (bounced * state.agent_pos + (~bounced) * fwd_pos).astype(jnp.uint32)
   396        15   13427356.0 895157.1      0.8          collision = jnp.all(fwd_pos[0] == fwd_pos[1])
   397                                           
   398                                                   # No collision = No movement. This matches original Overcooked env.
   399        30    3071640.0 102388.0      0.2          alice_pos = jnp.where(
   400        15       3121.0    208.1      0.0              collision,
   401        15    3142948.0 209529.9      0.2              state.agent_pos[0],                     # collision and Bob bounced
   402        15    2775786.0 185052.4      0.2              fwd_pos[0],
   403                                                   )
   404        30    2203353.0  73445.1      0.1          bob_pos = jnp.where(
   405        15       2152.0    143.5      0.0              collision,
   406        15    2748449.0 183229.9      0.2              state.agent_pos[1],                     # collision and Alice bounced
   407        15    2549648.0 169976.5      0.1              fwd_pos[1],
   408                                                   )
   409                                           
   410                                                   # Prevent swapping places (i.e. passing through each other)
   411        30    2043353.0  68111.8      0.1          swap_places = jnp.logical_and(
   412        15    9658436.0 643895.7      0.5              jnp.all(fwd_pos[0] == state.agent_pos[1]),
   413        15   10338072.0 689204.8      0.6              jnp.all(fwd_pos[1] == state.agent_pos[0]),
   414                                                   )
   415        30    2230334.0  74344.5      0.1          alice_pos = jnp.where(
   416        15    5158210.0 343880.7      0.3              ~collision * swap_places,
   417        15    2900216.0 193347.7      0.2              state.agent_pos[0],
   418        15       3180.0    212.0      0.0              alice_pos
   419                                                   )
   420        30    2202481.0  73416.0      0.1          bob_pos = jnp.where(
   421        15    4710044.0 314002.9      0.3              ~collision * swap_places,
   422        15    2923283.0 194885.5      0.2              state.agent_pos[1],
   423        15       3689.0    245.9      0.0              bob_pos
   424                                                   )
   425                                           
   426        15   14953221.0 996881.4      0.8          fwd_pos = fwd_pos.at[0].set(alice_pos)
   427        15   15375950.0    1e+06      0.9          fwd_pos = fwd_pos.at[1].set(bob_pos)
   428        15    2020638.0 134709.2      0.1          agent_pos = fwd_pos.astype(jnp.uint32)
   429                                           
   430                                                   # Update agent direction
   431        15   10496986.0 699799.1      0.6          agent_dir_idx = ~is_move_action * state.agent_dir_idx + is_move_action * action
   432        15   17106237.0    1e+06      1.0          agent_dir = DIR_TO_VEC[agent_dir_idx]
   433                                           
   434                                                   # Handle interacts. Agent 1 first, agent 2 second, no collision handling.
   435                                                   # This matches the original Overcooked
   436        15    3082331.0 205488.7      0.2          fwd_pos = state.agent_pos + state.agent_dir
   437        15       8978.0    598.5      0.0          maze_map = state.maze_map
   438        15    3688748.0 245916.5      0.2          is_interact_action = (action == Actions.interact)
   439                                           
   440                                                   # Compute the effect of interact first, then apply it if needed
   441        15  528384790.0    4e+07     29.6          candidate_maze_map, alice_inv, alice_reward, alice_shaped_reward = self.process_interact(maze_map, state.wall_map, fwd_pos, state.agent_inv, 0)
   442        15    2668111.0 177874.1      0.1          alice_interact = is_interact_action[0]
   443        15    2615807.0 174387.1      0.1          bob_interact = is_interact_action[1]
   444                                           
   445        30    1040892.0  34696.4      0.1          maze_map = jax.lax.select(alice_interact,
   446        15       2065.0    137.7      0.0                                candidate_maze_map,
   447        15       1794.0    119.6      0.0                                maze_map)
   448        30    1047681.0  34922.7      0.1          alice_inv = jax.lax.select(alice_interact,
   449        15       2298.0    153.2      0.0                                alice_inv,
   450        15    2600700.0 173380.0      0.1                                state.agent_inv[0])
   451        15     949975.0  63331.7      0.1          alice_reward = jax.lax.select(alice_interact, alice_reward, 0.)
   452        15     930745.0  62049.7      0.1          alice_shaped_reward = jax.lax.select(alice_interact, alice_shaped_reward, 0.)
   453                                           
   454        15  605871893.0    4e+07     34.0          candidate_maze_map, bob_inv, bob_reward, bob_shaped_reward = self.process_interact(maze_map, state.wall_map, fwd_pos, state.agent_inv, 1)
   455        30    1105526.0  36850.9      0.1          maze_map = jax.lax.select(bob_interact,
   456        15       1963.0    130.9      0.0                                candidate_maze_map,
   457        15       1509.0    100.6      0.0                                maze_map)
   458        30     980441.0  32681.4      0.1          bob_inv = jax.lax.select(bob_interact,
   459        15       2143.0    142.9      0.0                                bob_inv,
   460        15    2675965.0 178397.7      0.1                                state.agent_inv[1])
   461        15     959083.0  63938.9      0.1          bob_reward = jax.lax.select(bob_interact, bob_reward, 0.)
   462        15     943656.0  62910.4      0.1          bob_shaped_reward = jax.lax.select(bob_interact, bob_shaped_reward, 0.)
   463                                           
   464                                           
   465        15    8136766.0 542451.1      0.5          agent_inv = jnp.array([alice_inv, bob_inv])
   466                                           
   467                                                   # Update agent component in maze_map
   468        15       9093.0    606.2      0.0          def _get_agent_updates(agent_dir_idx, agent_pos, agent_pos_prev, agent_idx):
   469                                                       agent = jnp.array([OBJECT_TO_INDEX['agent'], COLOR_TO_INDEX['red']+agent_idx*2, agent_dir_idx], dtype=jnp.uint8)
   470                                                       agent_x_prev, agent_y_prev = agent_pos_prev
   471                                                       agent_x, agent_y = agent_pos
   472                                                       return agent_x, agent_y, agent_x_prev, agent_y_prev, agent
   473                                           
   474        15     636069.0  42404.6      0.0          vec_update = jax.vmap(_get_agent_updates, in_axes=(0, 0, 0, 0))
   475        15   64755724.0    4e+06      3.6          agent_x, agent_y, agent_x_prev, agent_y_prev, agent_vec = vec_update(agent_dir_idx, agent_pos, agent_pos_prev, jnp.arange(self.num_agents))
   476        15    2249683.0 149978.9      0.1          empty = jnp.array([OBJECT_TO_INDEX['empty'], 0, 0], dtype=jnp.uint8)
   477                                           
   478                                                   # Compute padding, added automatically by map maker function
   479        15      15344.0   1022.9      0.0          height = self.obs_shape[1]
   480        15      21769.0   1451.3      0.0          padding = (state.maze_map.shape[0] - height) // 2
   481                                           
   482        15   30446691.0    2e+06      1.7          maze_map = maze_map.at[padding + agent_y_prev, padding + agent_x_prev, :].set(empty)
   483        15   28355470.0    2e+06      1.6          maze_map = maze_map.at[padding + agent_y, padding + agent_x, :].set(agent_vec)
   484                                           
   485                                                   # Update pot cooking status
   486        15       8318.0    554.5      0.0          def _cook_pots(pot):
   487                                                       pot_status = pot[-1]
   488                                                       is_cooking = jnp.array(pot_status <= POT_FULL_STATUS)
   489                                                       not_done = jnp.array(pot_status > POT_READY_STATUS)
   490                                                       pot_status = is_cooking * not_done * (pot_status-1) + (~is_cooking) * pot_status # defaults to zero if done
   491                                                       return pot.at[-1].set(pot_status)
   492                                           
   493        15    3358850.0 223923.3      0.2          pot_x = state.pot_pos[:, 0]
   494        15    3278552.0 218570.1      0.2          pot_y = state.pot_pos[:, 1]
   495        15   24129260.0    2e+06      1.4          pots = maze_map.at[padding + pot_y, padding + pot_x].get()
   496        15   92224161.0    6e+06      5.2          pots = jax.vmap(_cook_pots, in_axes=0)(pots)
   497        15   27668473.0    2e+06      1.6          maze_map = maze_map.at[padding + pot_y, padding + pot_x, :].set(pots)
   498                                           
   499        15    3847348.0 256489.9      0.2          reward = alice_reward + bob_reward
   500                                           
   501        15       2382.0    158.8      0.0          return (
   502        30     334442.0  11148.1      0.0              state.replace(
   503        15       2995.0    199.7      0.0                  agent_pos=agent_pos,
   504        15       2784.0    185.6      0.0                  agent_dir_idx=agent_dir_idx,
   505        15       2448.0    163.2      0.0                  agent_dir=agent_dir,
   506        15       2468.0    164.5      0.0                  agent_inv=agent_inv,
   507        15       1604.0    106.9      0.0                  maze_map=maze_map,
   508        15       1883.0    125.5      0.0                  terminal=False),
   509        15       2314.0    154.3      0.0              reward,
   510        15       3600.0    240.0      0.0              (alice_shaped_reward, bob_shaped_reward)
   511                                                   )

Total time: 2.44768 s
File: /home/luka/repo/JAXOvercooked/jax_marl/environments/overcooked_environment/overcooked.py
Function: step_env at line 102

Line #      Hits         Time  Per Hit   % Time  Line Contents
==============================================================
   102                                               @line_profiler.profile
   103                                               def step_env(
   104                                                       self,
   105                                                       key: chex.PRNGKey,
   106                                                       state: State,
   107                                                       actions: Dict[str, chex.Array],
   108                                               ) -> Tuple[Dict[str, chex.Array], State, Dict[str, float], Dict[str, bool], Dict]:
   109                                                   """Perform single timestep state transition."""
   110                                           
   111        15   15529249.0    1e+06      0.6          acts = self.action_set.take(indices=jnp.array([actions["agent_0"], actions["agent_1"]]))
   112                                           
   113        15 1785405440.0    1e+08     72.9          state, reward, shaped_rewards = self.step_agents(key, state, acts)
   114                                           
   115        15    3185867.0 212391.1      0.1          state = state.replace(time=state.time + 1)
   116                                           
   117        15    8491739.0 566115.9      0.3          done = self.is_terminal(state)
   118        15     246246.0  16416.4      0.0          state = state.replace(terminal=done)
   119                                           
   120        15  631906869.0    4e+07     25.8          obs = self.get_obs(state)
   121        15       7137.0    475.8      0.0          rewards = {"agent_0": reward, "agent_1": reward}
   122        15      10992.0    732.8      0.0          shaped_rewards = {"agent_0": shaped_rewards[0], "agent_1": shaped_rewards[1]}
   123        15       4898.0    326.5      0.0          dones = {"agent_0": done, "agent_1": done, "__all__": done}
   124                                           
   125        15       2642.0    176.1      0.0          return (
   126        15     622563.0  41504.2      0.0              lax.stop_gradient(obs),
   127        15    2259164.0 150610.9      0.1              lax.stop_gradient(state),
   128        15       2632.0    175.5      0.0              rewards,
   129        15       1922.0    128.1      0.0              dones,
   130        15       5790.0    386.0      0.0              {'shaped_reward': shaped_rewards},
   131                                                   )

  0.56 seconds - /home/luka/repo/JAXOvercooked/baselines/test.py:122 - _env_step
  1.78 seconds - /home/luka/repo/JAXOvercooked/jax_marl/environments/overcooked_environment/overcooked.py:362 - step_agents
  2.45 seconds - /home/luka/repo/JAXOvercooked/jax_marl/environments/overcooked_environment/overcooked.py:102 - step_env
